{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199f5742-0674-4765-b413-faa4946ec0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure logs folder exists in repo\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Timestamped log file name\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_path = f\"logs/run_{run_ts}.log\"\n",
    "\n",
    "# Configure logging (console + file)\n",
    "logger = logging.getLogger(\"etl_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()  \n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(fmt)\n",
    "\n",
    "\n",
    "fh = logging.FileHandler(log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(fmt)\n",
    "\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "logger.info(\"========== ETL RUN START ==========\")\n",
    "logger.info(f\"Log file: {log_path}\")\n",
    "logger.info(f\"Python: {sys.version.split()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1429dc1f-1504-4677-b7e3-6b9b51fe9729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyspark\n",
    "    logger.info(f\"PySpark version: {pyspark.__version__}\")\n",
    "except Exception as e:\n",
    "    logger.info(f\"PySpark not available (likely local Jupyter). Details: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    logger.info(f\"Workspace: {ctx.workspaceId().get()}\")\n",
    "    logger.info(f\"Cluster ID: {ctx.clusterId().get()}\")\n",
    "except Exception:\n",
    "    logger.info(\"Databricks context not available (ok if running outside Databricks).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6bf2db-67e7-4298-b7b9-4f4a8be7db9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "sorted(glob.glob(\"logs/run_*.log\"))[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5cb839c-1b3e-461e-9dc8-af49a2b268ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "logger.info(\"Reproducibility: seeds set (PYTHONHASHSEED=0, random=0, numpy=0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18b5a68-1071-4031-9b0f-804c6bcd27cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dd1724-4048-4111-b9a7-83645234c8fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def sha256_file(path: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "files = [\"data/menu_items.csv\", \"data/order_details.csv\"]\n",
    "hashes = {}\n",
    "\n",
    "for fp in files:\n",
    "    p = Path(fp)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {fp}\")\n",
    "    hashes[p.name] = sha256_file(fp)\n",
    "\n",
    "with open(\"data_hashes.json\", \"w\") as f:\n",
    "    json.dump(hashes, f, indent=2)\n",
    "\n",
    "logger.info(\"Wrote data_hashes.json with SHA-256 hashes:\")\n",
    "logger.info(hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63b3fbe-40b2-4789-82a2-7773aae0c60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"requirements.txt exists:\", os.path.exists(\"requirements.txt\"))\n",
    "print(\"data_hashes.json exists:\", os.path.exists(\"data_hashes.json\"))\n",
    "print(\"logs folder exists:\", os.path.exists(\"logs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4387c6f2-f43d-4c45-be9a-a6c22327cf9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_dir = \"/FileStore/tables/etl_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "logger.info(f\"ETL output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c8650d-a571-469e-b2a2-a5fd5989647a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(menu_path=\"data/menu_items.csv\", orders_path=\"data/order_details.csv\"):\n",
    "    logger.info(f\"Loading menu from {menu_path}\")\n",
    "    logger.info(f\"Loading orders from {orders_path}\")\n",
    "    menu = pd.read_csv(menu_path)\n",
    "    orders = pd.read_csv(orders_path)\n",
    "    logger.info(f\"Loaded menu rows: {len(menu)} | orders rows: {len(orders)}\")\n",
    "    return menu, orders\n",
    "\n",
    "menu_df, orders_df = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36504f6-3973-4584-9d9e-f1d4a0452423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(menu: pd.DataFrame, orders: pd.DataFrame):\n",
    "    # Normalize column names\n",
    "    menu = menu.copy()\n",
    "    orders = orders.copy()\n",
    "    menu.columns = menu.columns.str.strip().str.lower()\n",
    "    orders.columns = orders.columns.str.strip().str.lower()\n",
    "\n",
    "    \n",
    "    for col in menu.select_dtypes(include=\"object\").columns:\n",
    "        menu[col] = menu[col].astype(str).str.strip()\n",
    "\n",
    "    for col in orders.select_dtypes(include=\"object\").columns:\n",
    "        orders[col] = orders[col].astype(str).str.strip()\n",
    "\n",
    "    \n",
    "    if \"price\" in menu.columns:\n",
    "        menu[\"price\"] = pd.to_numeric(menu[\"price\"], errors=\"coerce\")\n",
    "\n",
    "    if \"order_date\" in orders.columns:\n",
    "        orders[\"order_date\"] = pd.to_datetime(orders[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "    if \"order_time\" in orders.columns:\n",
    "        # Keep as string or parse to time; we’ll use it if needed\n",
    "        orders[\"order_time\"] = orders[\"order_time\"].astype(str).str.strip()\n",
    "\n",
    "    return menu, orders\n",
    "\n",
    "menu_df, orders_df = clean_data(menu_df, orders_df)\n",
    "logger.info(\"Data cleaning complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e80c807-1437-4898-b272-354a3b92d355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def join_data(menu: pd.DataFrame, orders: pd.DataFrame):\n",
    "   \n",
    "    assert \"menu_item_id\" in menu.columns, \"menu_items missing menu_item_id\"\n",
    "    assert \"item_id\" in orders.columns, \"order_details missing item_id\"\n",
    "\n",
    "    joined = orders.merge(\n",
    "        menu,\n",
    "        left_on=\"item_id\",\n",
    "        right_on=\"menu_item_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Joined rows: {len(joined)}\")\n",
    "    return joined\n",
    "\n",
    "joined_df = join_data(menu_df, orders_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b2bbf5-015d-4412-b36c-8fdcfed94786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_tidy(df: pd.DataFrame):\n",
    "    \n",
    "    keep_cols = []\n",
    "    for c in [\"order_id\", \"order_date\", \"order_time\", \"item_name\", \"category\", \"price\", \"quantity\"]:\n",
    "        if c in df.columns:\n",
    "            keep_cols.append(c)\n",
    "\n",
    "    tidy = df[keep_cols].copy()\n",
    "\n",
    "   \n",
    "    if \"order_date\" in tidy.columns and \"order_time\" in tidy.columns:\n",
    "        tidy[\"order_datetime\"] = pd.to_datetime(\n",
    "            tidy[\"order_date\"].astype(str) + \" \" + tidy[\"order_time\"].astype(str),\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "    elif \"order_date\" in tidy.columns:\n",
    "        tidy[\"order_datetime\"] = pd.to_datetime(tidy[\"order_date\"], errors=\"coerce\")\n",
    "\n",
    "    return tidy\n",
    "\n",
    "tidy_df = make_tidy(joined_df)\n",
    "logger.info(f\"Tidy columns: {list(tidy_df.columns)}\")\n",
    "logger.info(f\"Tidy rows: {len(tidy_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec58ea2-56df-4427-9e92-f09fa7e0a203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(tidy: pd.DataFrame):\n",
    "   \n",
    "    if \"quantity\" not in tidy.columns:\n",
    "        tidy = tidy.copy()\n",
    "        tidy[\"quantity\"] = 1\n",
    "\n",
    "    \n",
    "    if \"price\" in tidy.columns:\n",
    "        tidy = tidy.copy()\n",
    "        tidy[\"revenue\"] = tidy[\"quantity\"] * tidy[\"price\"]\n",
    "    else:\n",
    "        tidy[\"revenue\"] = None\n",
    "\n",
    "    top_5_items = (\n",
    "        tidy.groupby(\"item_name\")[\"quantity\"]\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(5)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"quantity\": \"total_quantity\"})\n",
    "    )\n",
    "\n",
    "    revenue_by_category = None\n",
    "    if \"category\" in tidy.columns and tidy[\"revenue\"].notna().any():\n",
    "        revenue_by_category = (\n",
    "            tidy.groupby(\"category\")[\"revenue\"]\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    busiest_hour = None\n",
    "    if \"order_datetime\" in tidy.columns:\n",
    "        hourly = tidy.dropna(subset=[\"order_datetime\"]).copy()\n",
    "        hourly[\"hour\"] = hourly[\"order_datetime\"].dt.hour\n",
    "        busiest_hour = (\n",
    "            hourly.groupby(\"hour\")\n",
    "            .size()\n",
    "            .reset_index(name=\"order_count\")\n",
    "            .sort_values(\"order_count\", ascending=False)\n",
    "            .head(1)\n",
    "        )\n",
    "\n",
    "    return top_5_items, revenue_by_category, busiest_hour\n",
    "\n",
    "top_5_items_df, revenue_by_category_df, busiest_hour_df = compute_metrics(tidy_df)\n",
    "\n",
    "logger.info(\"Computed metrics\")\n",
    "logger.info(f\"Top 5 items rows: {len(top_5_items_df)}\")\n",
    "if revenue_by_category_df is not None:\n",
    "    logger.info(f\"Revenue by category rows: {len(revenue_by_category_df)}\")\n",
    "if busiest_hour_df is not None:\n",
    "    logger.info(f\"Busiest hour rows: {len(busiest_hour_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8248f77f-98f2-4c43-89e0-86579e0b8336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metrics_file = f\"{output_dir}/metrics_{run_ts}.csv\"\n",
    "tidy_file = f\"{output_dir}/tidy_{run_ts}.csv\"\n",
    "\n",
    "# Save tidy output\n",
    "tidy_df.to_csv(tidy_file, index=False)\n",
    "\n",
    "\n",
    "with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"TOP_5_ITEMS\\n\")\n",
    "    top_5_items_df.to_csv(f, index=False)\n",
    "    f.write(\"\\nREVENUE_BY_CATEGORY\\n\")\n",
    "    if revenue_by_category_df is not None:\n",
    "        revenue_by_category_df.to_csv(f, index=False)\n",
    "    f.write(\"\\nBUSIEST_HOUR\\n\")\n",
    "    if busiest_hour_df is not None:\n",
    "        busiest_hour_df.to_csv(f, index=False)\n",
    "\n",
    "logger.info(f\"Saved tidy output: {tidy_file}\")\n",
    "logger.info(f\"Saved metrics output: {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811a553a-68fc-402b-860b-b79a071d5d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metrics_file = f\"{output_dir}/metrics_{run_ts}.csv\"\n",
    "tidy_file = f\"{output_dir}/tidy_{run_ts}.csv\"\n",
    "\n",
    "# Save tidy output\n",
    "tidy_df.to_csv(tidy_file, index=False)\n",
    "\n",
    "# Save metrics (combine into one CSV with sections)\n",
    "with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"TOP_5_ITEMS\\n\")\n",
    "    top_5_items_df.to_csv(f, index=False)\n",
    "    f.write(\"\\nREVENUE_BY_CATEGORY\\n\")\n",
    "    if revenue_by_category_df is not None:\n",
    "        revenue_by_category_df.to_csv(f, index=False)\n",
    "    f.write(\"\\nBUSIEST_HOUR\\n\")\n",
    "    if busiest_hour_df is not None:\n",
    "        busiest_hour_df.to_csv(f, index=False)\n",
    "\n",
    "logger.info(f\"Saved tidy output: {tidy_file}\")\n",
    "logger.info(f\"Saved metrics output: {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c09fade-2b62-4585-a01a-1085786ca4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic ETL validation tests\n",
    "assert not tidy_df.empty, \"Tidy output is empty\"\n",
    "assert \"order_id\" in tidy_df.columns, \"Missing order_id in tidy output\"\n",
    "assert \"item_name\" in tidy_df.columns, \"Missing item_name in tidy output\"\n",
    "assert \"price\" in tidy_df.columns, \"Missing price in tidy output\"\n",
    "\n",
    "assert not top_5_items_df.empty, \"Top 5 items metric is empty\"\n",
    "assert \"item_name\" in top_5_items_df.columns, \"Top 5 missing item_name\"\n",
    "assert \"total_quantity\" in top_5_items_df.columns, \"Top 5 missing total_quantity\"\n",
    "\n",
    "logger.info(\"All Part D assertions passed ✅\")\n",
    "logger.info(\"========== ETL RUN END ==========\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc758c0d-46b2-4d71-b946-d6c36da80761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"dbfs:/FileStore/tables/etl_output\"\n",
    "dbutils.fs.mkdirs(output_dir)\n",
    "\n",
    "logger.info(f\"Ensured DBFS output dir exists: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f04df8-c9df-42bc-a99c-636f5f3f7eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Output directory set to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72a5efc-94f3-4cfe-8d2e-6768c82b5eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metrics_file = f\"{output_dir}/metrics_{run_ts}.csv\"\n",
    "tidy_file = f\"{output_dir}/tidy_{run_ts}.csv\"\n",
    "\n",
    "tidy_df.to_csv(tidy_file, index=False)\n",
    "\n",
    "with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"TOP_5_ITEMS\\n\")\n",
    "    top_5_items_df.to_csv(f, index=False)\n",
    "    f.write(\"\\nREVENUE_BY_CATEGORY\\n\")\n",
    "    if revenue_by_category_df is not None:\n",
    "        revenue_by_category_df.to_csv(f, index=False)\n",
    "    f.write(\"\\nBUSIEST_HOUR\\n\")\n",
    "    if busiest_hour_df is not None:\n",
    "        busiest_hour_df.to_csv(f, index=False)\n",
    "\n",
    "logger.info(f\"Saved tidy output: {tidy_file}\")\n",
    "logger.info(f\"Saved metrics output: {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ce5388-2e31-42ca-980a-182520fdaf87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "glob.glob(\"outputs/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8bfed56-fa60-4aa9-af1e-c14b78054099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Top 5 items preview:\")\n",
    "logger.info(top_5_items_df.head().to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_4_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
