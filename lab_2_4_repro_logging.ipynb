{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199f5742-0674-4765-b413-faa4946ec0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure logs folder exists in repo\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Timestamped log file name\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_path = f\"logs/run_{run_ts}.log\"\n",
    "\n",
    "# Configure logging (console + file)\n",
    "logger = logging.getLogger(\"etl_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()  \n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(fmt)\n",
    "\n",
    "\n",
    "fh = logging.FileHandler(log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(fmt)\n",
    "\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "logger.info(\"========== ETL RUN START ==========\")\n",
    "logger.info(f\"Log file: {log_path}\")\n",
    "logger.info(f\"Python: {sys.version.split()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1429dc1f-1504-4677-b7e3-6b9b51fe9729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyspark\n",
    "    logger.info(f\"PySpark version: {pyspark.__version__}\")\n",
    "except Exception as e:\n",
    "    logger.info(f\"PySpark not available (likely local Jupyter). Details: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    logger.info(f\"Workspace: {ctx.workspaceId().get()}\")\n",
    "    logger.info(f\"Cluster ID: {ctx.clusterId().get()}\")\n",
    "except Exception:\n",
    "    logger.info(\"Databricks context not available (ok if running outside Databricks).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6bf2db-67e7-4298-b7b9-4f4a8be7db9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "sorted(glob.glob(\"logs/run_*.log\"))[-3:]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab_2_4_repro_logging",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
