{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6b9d7a-95f0-4947-b535-fc7d6c8426f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering with Pipelines (Assignment 4.3)\n",
    "\n",
    "In this notebook, I prepare the curated STEDI Silver dataset for machine learning by engineering features that models can interpret. This includes creating train/test splits, scaling numeric features, one-hot encoding categorical features, and combining all preprocessing steps into a single reproducible scikit-learn Pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76aea4a0-966a-494f-9327-5497170e73ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Import Libraries and Load Curated Dataset\n",
    "\n",
    "In this step, I load the curated Silver dataset (`labeled_step_test`) from Databricks using Spark and convert it into a pandas DataFrame. Pandas is required because scikit-learn operates on in-memory data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579edc07-1506-48a9-a445-9f063b118139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_spark = spark.table(\"labeled_step_test\")\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4e1a3a-10a2-4dd1-a0f4-b7a9e5f7e698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Define Feature Columns\n",
    "\n",
    "Here I define which columns will be used as model inputs (features) and which column represents the label. Numeric and categorical features are separated so they can be preprocessed differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdebf2d-7e42-4772-b859-944dc4f0d42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Numeric features\n",
    "feature_cols_numeric = [\"distance_cm\"]\n",
    "\n",
    "# Categorical features\n",
    "feature_cols_categorical = [\"sensor_type\", \"device_id\"]\n",
    "\n",
    "# Label\n",
    "label_col = \"step_label\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ccd7b5-82ba-4af1-91ab-38e6392568fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Create Train/Test Split\n",
    "\n",
    "The dataset is split into training and testing sets. Stratification is used to preserve the proportion of step and no_step labels in both sets, which helps prevent bias during model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7864d89-5757-4e1f-a051-6ab210d08fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[feature_cols_numeric + feature_cols_categorical]\n",
    "y = df[label_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2adcc4-3036-4a72-8cd9-968bfe021497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Define Preprocessing Transformers\n",
    "\n",
    "Numeric features are scaled using StandardScaler, while categorical features are one-hot encoded. These transformations ensure features are comparable and usable by machine-learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049623df-c745-4ff8-a15f-9623dd0ff00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_cols_numeric),\n",
    "        (\"cat\", categorical_transformer, feature_cols_categorical)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad43aac-e82e-4db4-847d-2b20ba05fa1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Build Feature Engineering Pipeline\n",
    "\n",
    "The preprocessing steps are combined into a single scikit-learn Pipeline. This ensures that the same transformations are applied consistently every time the pipeline is run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79ecbd1-c33a-4d6c-8af6-cd2085139628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1501736c-45f7-47c6-9829-76c168eb4a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Fit Pipeline and Transform Data\n",
    "\n",
    "The pipeline is fit on the training data only, then applied to both training and test sets. This prevents data leakage and ensures fair evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c8b393-6c1f-4f09-9896-01fdc2bd740a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train)\n",
    "\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "X_train_transformed, X_test_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f49b4d3-f951-4bdb-9459-9f3bc16cc1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Save Pipeline and Processed Features\n",
    "\n",
    "The trained pipeline and transformed datasets are saved using joblib so they can be reused later for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09360018-205a-4d48-8d76-47c7e09de333",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 7: Save Pipeline and Processed Features"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Save locally first\n",
    "joblib.dump(pipeline, \"/tmp/stedi_feature_pipeline.pkl\")\n",
    "joblib.dump(X_train_transformed, \"/tmp/X_train_transformed.pkl\")\n",
    "joblib.dump(X_test_transformed, \"/tmp/X_test_transformed.pkl\")\n",
    "joblib.dump(y_train, \"/tmp/y_train.pkl\")\n",
    "joblib.dump(y_test, \"/tmp/y_test.pkl\")\n",
    "\n",
    "# Show saved files\n",
    "os.listdir(\"/tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e90bcc-a87c-4f38-8181-b61a703289fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ethics Reflection\n",
    "\n",
    "Using a consistent and reproducible feature pipeline helps prevent unfairness by ensuring that all data is processed in the same way every time, reducing hidden bias caused by inconsistent preprocessing. Scaling and encoding features consistently also prevents certain inputs from having unintended influence on model predictions. Reproducible pipelines make it easier to audit and validate results, which increases accountability. Spiritually, Doctrine and Covenants 130:20–21 teaches that blessings follow dependable laws, reminding me that fairness and consistency—both in data and in life—come from following reliable, repeatable principles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9d1e48-3d26-4c7a-940a-c79759c1d212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "ARTIFACT_DIR = \"/Workspace/Users/dec816@ensign.edu/csai382_lab_2_4_-DesmondChaparadza-/etl_pipeline\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# Copy from /tmp (already saved) → repo folder\n",
    "joblib.dump(pipeline, f\"{ARTIFACT_DIR}/stedi_feature_pipeline.pkl\")\n",
    "joblib.dump(X_train_transformed, f\"{ARTIFACT_DIR}/X_train_transformed.pkl\")\n",
    "joblib.dump(X_test_transformed, f\"{ARTIFACT_DIR}/X_test_transformed.pkl\")\n",
    "joblib.dump(y_train, f\"{ARTIFACT_DIR}/y_train.pkl\")\n",
    "joblib.dump(y_test, f\"{ARTIFACT_DIR}/y_test.pkl\")\n",
    "\n",
    "# Verify\n",
    "os.listdir(ARTIFACT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ac5894-9265-49f8-b726-c38f94410885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(os.listdir(ARTIFACT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b0cd8b-6d59-455c-b4c5-565eca718046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, joblib\n",
    "\n",
    "# Save inside your Workspace repo folder (this is what worked for you already)\n",
    "SAVE_DIR = \"/Workspace/Users/dec816@ensign.edu/csai382_lab_2_4_-DesmondChaparadza-/etl_pipeline\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "joblib.dump(pipeline, f\"{SAVE_DIR}/stedi_feature_pipeline.pkl\")\n",
    "joblib.dump(X_train_transformed, f\"{SAVE_DIR}/X_train_transformed.pkl\")\n",
    "joblib.dump(X_test_transformed, f\"{SAVE_DIR}/X_test_transformed.pkl\")\n",
    "joblib.dump(y_train, f\"{SAVE_DIR}/y_train.pkl\")\n",
    "joblib.dump(y_test, f\"{SAVE_DIR}/y_test.pkl\")\n",
    "\n",
    "print(\"Saved files:\", os.listdir(SAVE_DIR))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.3 Feature Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
