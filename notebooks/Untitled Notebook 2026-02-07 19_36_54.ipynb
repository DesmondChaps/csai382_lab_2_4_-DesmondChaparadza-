{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6573ba-df05-4395-b4ce-c02c118cd9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Databricks Notebook: Model Training with Hyperparameter Tuning\n",
    "Overview\n",
    "\n",
    "\n",
    "This notebook performs hyperparameter tuning for two machine learning models (Logistic Regression and Random Forest) using previously transformed features from the STEDI project. The notebook loads saved pipeline artifacts, converts features to appropriate formats, performs grid search, compares models, and saves the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db96bb8-6d50-4c70-982b-c0fa764552d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up CORRECT Databricks paths\n",
    "# In Databricks, use paths without /dbfs/ prefix for joblib\n",
    "base_path = \"/etl_pipeline\"\n",
    "pipeline_path = f\"{base_path}/stedi_feature_pipeline.pkl\"\n",
    "X_train_path = f\"{base_path}/X_train_transformed.pkl\"\n",
    "X_test_path = f\"{base_path}/X_test_transformed.pkl\"\n",
    "y_train_path = f\"{base_path}/y_train.pkl\"\n",
    "y_test_path = f\"{base_path}/y_test.pkl\"\n",
    "\n",
    "# Verify files exist before loading\n",
    "import os\n",
    "print(\"Checking if files exist...\")\n",
    "print(f\"Pipeline file exists: {os.path.exists(pipeline_path)}\")\n",
    "print(f\"X_train file exists: {os.path.exists(X_train_path)}\")\n",
    "print(f\"X_test file exists: {os.path.exists(X_test_path)}\")\n",
    "print(f\"y_train file exists: {os.path.exists(y_train_path)}\")\n",
    "print(f\"y_test file exists: {os.path.exists(y_test_path)}\")\n",
    "\n",
    "# Load the saved pipeline and datasets\n",
    "print(\"\\nLoading saved pipeline and datasets...\")\n",
    "try:\n",
    "    feature_pipeline = joblib.load(pipeline_path)\n",
    "    X_train_transformed = joblib.load(X_train_path)\n",
    "    X_test_transformed = joblib.load(X_test_path)\n",
    "    y_train = joblib.load(y_train_path)\n",
    "    y_test = joblib.load(y_test_path)\n",
    "    print(\"✓ Loading complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading files: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Check if the files were created in the previous assignment\")\n",
    "    print(\"2. Verify the path: /etl_pipeline/\")\n",
    "    print(\"3. List files in the directory:\")\n",
    "    try:\n",
    "        print(os.listdir(\"/etl_pipeline\"))\n",
    "    except:\n",
    "        print(\"Could not list directory contents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b2dfc9-b1f9-445c-b63d-dd535477e817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Feature Conversion\n",
    "\n",
    "Convert transformed feature arrays into clean numeric 2-D NumPy arrays. This step is essential because the transformed arrays may be sparse, object-typed, or oddly shaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317d8258-674a-44b3-8742-d8e11979b70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_numeric_2d(array_like):\n",
    "    \"\"\"\n",
    "    Convert transformed feature arrays into clean numeric 2-D NumPy arrays.\n",
    "    Handles sparse matrices, DataFrames, and other possible outputs from the pipeline.\n",
    "    \"\"\"\n",
    "    # Convert to dense array if sparse\n",
    "    if hasattr(array_like, 'toarray'):\n",
    "        array_like = array_like.toarray()\n",
    "    \n",
    "    # Convert to numpy array if DataFrame\n",
    "    if hasattr(array_like, 'values'):\n",
    "        array_like = array_like.values\n",
    "    \n",
    "    # Ensure it's a numpy array\n",
    "    array_like = np.array(array_like)\n",
    "    \n",
    "    # Handle 1-D arrays by reshaping to 2-D\n",
    "    if array_like.ndim == 1:\n",
    "        array_like = array_like.reshape(-1, 1)\n",
    "    \n",
    "    # Convert object dtype to float if needed\n",
    "    if array_like.dtype == object:\n",
    "        try:\n",
    "            array_like = array_like.astype(float)\n",
    "        except ValueError:\n",
    "            # If conversion fails, use one-hot encoding for categorical columns\n",
    "            from sklearn.preprocessing import OneHotEncoder\n",
    "            encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "            array_like = encoder.fit_transform(array_like)\n",
    "    \n",
    "    # Final check for NaN values\n",
    "    if np.isnan(array_like).any():\n",
    "        array_like = np.nan_to_num(array_like)\n",
    "    \n",
    "    return array_like\n",
    "\n",
    "# Apply conversion to transformed datasets\n",
    "print(\"Converting transformed features to numeric 2-D arrays...\")\n",
    "X_train = convert_to_numeric_2d(X_train_transformed)\n",
    "X_test = convert_to_numeric_2d(X_test_transformed)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"\\n=== Dataset Shapes ===\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Verify data types\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(f\"X_train dtype: {X_train.dtype}\")\n",
    "print(f\"X_test dtype: {X_test.dtype}\")\n",
    "print(f\"y_train dtype: {y_train.dtype}\")\n",
    "print(f\"y_test dtype: {y_test.dtype}\")\n",
    "\n",
    "# Sample preview\n",
    "print(\"\\n=== Sample Data (first 3 rows) ===\")\n",
    "print(\"X_train sample:\")\n",
    "print(X_train[:3])\n",
    "print(\"\\ny_train sample:\")\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64abd7b-5cce-4e32-9ed6-1d0e7324037b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Logistic Regression Tuning\n",
    "\n",
    "\n",
    "\n",
    "Perform hyperparameter tuning for Logistic Regression using GridSearchCV with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372ae538-4d44-4ef4-97ba-bf411fb691f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "print(\"Starting Logistic Regression Hyperparameter Tuning...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers that support L1\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Create and fit GridSearchCV\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=lr_model,\n",
    "    param_grid=param_grid_lr,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit GridSearchCV\n",
    "print(\"Performing grid search for Logistic Regression...\")\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# Record end time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Logistic Regression Tuning Results ===\")\n",
    "print(f\"Best parameters: {grid_search_lr.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search_lr.best_score_:.4f}\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_lr = grid_search_lr.predict(X_test)\n",
    "test_accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Test set accuracy: {test_accuracy_lr:.4f}\")\n",
    "\n",
    "# Store best model\n",
    "best_lr_model = grid_search_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71b7fc7-d712-480b-bc17-d9c1f6a3012c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Random Forest Tuning\n",
    "\n",
    "\n",
    "Perform hyperparameter tuning for Random Forest using GridSearchCV with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a6f71b-6dfe-4f5a-a719-d825b609e11e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"\\nStarting Random Forest Hyperparameter Tuning...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [10, 20, 30, None],  # Maximum depth of trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples at a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features for best split\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# Create and fit GridSearchCV\n",
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit GridSearchCV\n",
    "print(\"Performing grid search for Random Forest...\")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Record end time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== Random Forest Tuning Results ===\")\n",
    "print(f\"Best parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search_rf.best_score_:.4f}\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_rf = grid_search_rf.predict(X_test)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Test set accuracy: {test_accuracy_rf:.4f}\")\n",
    "\n",
    "# Store best model\n",
    "best_rf_model = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f88d5873-fb47-4c8b-b843-f084b9f3e773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "5. Model Comparison\n",
    "\n",
    "\n",
    "Compare the tuned models and select the best one based on cross-validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c64db7-3c77-46ea-8793-3b5517f3404f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comparison dictionary as required\n",
    "model_comparison = {\n",
    "    'LogisticRegression': {\n",
    "        'best_params': grid_search_lr.best_params_,\n",
    "        'best_cv_score': grid_search_lr.best_score_,\n",
    "        'test_accuracy': test_accuracy_lr,\n",
    "        'model': best_lr_model\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'best_params': grid_search_rf.best_params_,\n",
    "        'best_cv_score': grid_search_rf.best_score_,\n",
    "        'test_accuracy': test_accuracy_rf,\n",
    "        'model': best_rf_model\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<20} {'CV Score':<15} {'Test Accuracy':<15} {'Selected':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, results in model_comparison.items():\n",
    "    print(f\"{model_name:<20} {results['best_cv_score']:<15.4f} {results['test_accuracy']:<15.4f}\")\n",
    "\n",
    "# Select best model based on cross-validation score\n",
    "if model_comparison['LogisticRegression']['best_cv_score'] > model_comparison['RandomForest']['best_cv_score']:\n",
    "    best_model_name = 'LogisticRegression'\n",
    "    best_model = best_lr_model\n",
    "    print(f\"\\n✓ Selected best model: {best_model_name} (higher CV score)\")\n",
    "else:\n",
    "    best_model_name = 'RandomForest'\n",
    "    best_model = best_rf_model\n",
    "    print(f\"\\n✓ Selected best model: {best_model_name} (higher CV score)\")\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, results in model_comparison.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Best CV Score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"  Parameters: {results['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02d4da5-7698-425d-ab8e-de871413fdda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Model Saving\n",
    "\n",
    "Save the selected best model and comparison results for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9bd339-3880-4dd9-99c5-3ae4d07bb590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n=== Saving Selected Model ===\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = \"/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for versioning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the best model\n",
    "model_filename = f\"{models_dir}/stedi_best_model_{timestamp}.pkl\"\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "# Also save a version without timestamp for easy reference\n",
    "latest_model_filename = f\"{models_dir}/stedi_best_model_latest.pkl\"\n",
    "joblib.dump(best_model, latest_model_filename)\n",
    "\n",
    "print(f\"✓ Best model ({best_model_name}) saved to:\")\n",
    "print(f\"  - Versioned: {model_filename}\")\n",
    "print(f\"  - Latest: {latest_model_filename}\")\n",
    "\n",
    "# Save the model comparison results\n",
    "comparison_filename = f\"{models_dir}/model_comparison_{timestamp}.pkl\"\n",
    "joblib.dump(model_comparison, comparison_filename)\n",
    "print(f\"✓ Model comparison saved to: {comparison_filename}\")\n",
    "\n",
    "# Verify the saved model can be loaded\n",
    "print(\"\\nVerifying saved model can be loaded...\")\n",
    "loaded_model = joblib.load(latest_model_filename)\n",
    "print(f\"✓ Model loaded successfully. Type: {type(loaded_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf64cfd0-8e4a-4e78-9faf-767115cd1bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Evaluation & Ethics Reflection\n",
    "\n",
    "Provide comprehensive evaluation metrics and ethical considerations for the STEDI project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a63e927-ffa9-40e5-9021-0c20e616380f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Evaluation & Ethics Reflection ===\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import metrics for comprehensive evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate predictions from best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "print(\"\\n1. PERFORMANCE EVALUATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC if probabilities are available\n",
    "if y_pred_proba is not None:\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    except:\n",
    "        print(\"ROC-AUC calculation skipped (requires probability estimates)\")\n",
    "\n",
    "print(\"\\n2. ETHICAL CONSIDERATIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "ethical_reflection = \"\"\"\n",
    "For the STEDI project, which involves human activity/fall detection:\n",
    "\n",
    "a) **Fairness & Bias**: \n",
    "   - The model must perform equally well across different demographic groups\n",
    "   - Training data should represent diverse age groups, mobility levels, and body types\n",
    "   - Regular bias audits should be conducted\n",
    "\n",
    "b) **Privacy**: \n",
    "   - Motion sensor data must be anonymized and secured\n",
    "   - Inference should happen locally when possible to protect user data\n",
    "   - Clear data retention policies are needed\n",
    "\n",
    "c) **Safety**: \n",
    "   - False negatives (missed falls) are more dangerous than false positives\n",
    "   - Model confidence thresholds should prioritize recall over precision\n",
    "   - A human-in-the-loop system for critical decisions is recommended\n",
    "\n",
    "d) **Transparency**: \n",
    "   - Users should understand how predictions are made\n",
    "   - Model limitations should be clearly communicated\n",
    "   - Fallback mechanisms for model uncertainty are essential\n",
    "\n",
    "e) **Continual Monitoring**: \n",
    "   - Model performance should be monitored in production\n",
    "   - Concept drift detection for changing user behaviors\n",
    "   - Regular retraining with new, representative data\n",
    "\"\"\"\n",
    "\n",
    "print(ethical_reflection)\n",
    "\n",
    "print(\"\\n3. DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "1. **Model Serving**: Deploy as a REST API using MLflow or Databricks Model Serving\n",
    "2. **Monitoring**: Implement tracking of:\n",
    "   - Prediction latency\n",
    "   - Model accuracy drift\n",
    "   - Feature distribution shifts\n",
    "3. **Alerting**: Set up alerts for:\n",
    "   - Sudden drop in performance\n",
    "   - Increased false negative rate\n",
    "   - System downtime\n",
    "4. **Version Control**: Maintain model version history with:\n",
    "   - Training data snapshots\n",
    "   - Hyperparameter configurations\n",
    "   - Performance metrics\n",
    "5. **Compliance**: Ensure compliance with healthcare regulations if applicable\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "# Create a simple visualization of model performance\n",
    "print(\"\\n4. VISUAL SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "\n",
    "# Plot 2: Model Comparison Bar Chart\n",
    "models = list(model_comparison.keys())\n",
    "cv_scores = [model_comparison[m]['best_cv_score'] for m in models]\n",
    "test_scores = [model_comparison[m]['test_accuracy'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, cv_scores, width, label='CV Score', color='skyblue')\n",
    "axes[1].bar(x + width/2, test_scores, width, label='Test Accuracy', color='lightcoral')\n",
    "axes[1].set_title('Model Performance Comparison')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "\n",
    "print(\"\\n✓ Notebook execution completed successfully!\")\n",
    "print(\"✓ Best model saved and ready for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2026-02-07 19_36_54",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
